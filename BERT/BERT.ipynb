{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3841a318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:02.169783Z",
     "start_time": "2025-05-20T16:20:00.019949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\loris\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fcb0697",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:02.176836Z",
     "start_time": "2025-05-20T16:20:02.170789Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alep9\\Anaconda3\\envs\\bert\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "import random\n",
    "#from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from tabulate import tabulate\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification, BertConfig,BertTokenizer,get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f67a54",
   "metadata": {},
   "source": [
    "Estrazione file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed18caa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:02.820307Z",
     "start_time": "2025-05-20T16:20:02.177841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (25000, 2)\n",
      "Test shape: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "zip_path = 'IMDB.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_path) as z:\n",
    "    with z.open('train.csv') as f_train, z.open('test.csv') as f_test:\n",
    "        train_df = pd.read_csv(f_train)\n",
    "        test_df = pd.read_csv(f_test)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb801d4",
   "metadata": {},
   "source": [
    "Prendiamo un sample di dati per non avere difficoltà con la memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a30551db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:02.829855Z",
     "start_time": "2025-05-20T16:20:02.821311Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_df[:3000]\n",
    "test_data = test_df[:750]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9800f1b6618a11f",
   "metadata": {},
   "source": [
    "## Fase di ETL\n",
    "Iniziamo ripulendo il dataset da tutti i caratteri speciali, emoji e tag html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d86c22d1048c6423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:02.836831Z",
     "start_time": "2025-05-20T16:20:02.829855Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alep9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,]+\",\" \", text)\n",
    "    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n",
    "    #text = re.sub(r\"http\", \"\",text)\n",
    "    \n",
    "    html=re.compile(r'<.*?>') \n",
    "    \n",
    "    text = html.sub(r'',text) #Removing html tags\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p,'') #Removing punctuations\n",
    "        \n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    \n",
    "    text = \" \".join(text) #removing stopwords\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text) #Removing emojis\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf57059f29a3da49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:04.424200Z",
     "start_time": "2025-05-20T16:20:02.838112Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alep9\\AppData\\Local\\Temp\\ipykernel_19860\\3025512271.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['text'] = train_data['text'].apply(lambda x: clean_text(x))\n",
      "C:\\Users\\alep9\\AppData\\Local\\Temp\\ipykernel_19860\\3025512271.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['text'] = test_data['text'].apply(lambda x: clean_text(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════════╤═════════════╕\n",
      "│ Tokens       │   Token IDs │\n",
      "╞══════════════╪═════════════╡\n",
      "│ now          │        2085 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ,            │        1010 │\n",
      "├──────────────┼─────────────┤\n",
      "│ deny         │        9772 │\n",
      "├──────────────┼─────────────┤\n",
      "│ purchased    │        4156 │\n",
      "├──────────────┼─────────────┤\n",
      "│ e            │        1041 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ##bay        │       15907 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ,            │        1010 │\n",
      "├──────────────┼─────────────┤\n",
      "│ high         │        2152 │\n",
      "├──────────────┼─────────────┤\n",
      "│ expectations │       10908 │\n",
      "├──────────────┼─────────────┤\n",
      "│ incredible   │        9788 │\n",
      "├──────────────┼─────────────┤\n",
      "│ print        │        6140 │\n",
      "├──────────────┼─────────────┤\n",
      "│ work         │        2147 │\n",
      "├──────────────┼─────────────┤\n",
      "│ master       │        3040 │\n",
      "├──────────────┼─────────────┤\n",
      "│ comedy       │        4038 │\n",
      "├──────────────┼─────────────┤\n",
      "│ enjoy        │        5959 │\n",
      "├──────────────┼─────────────┤\n",
      "│ however      │        2174 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ,            │        1010 │\n",
      "├──────────────┼─────────────┤\n",
      "│ soon         │        2574 │\n",
      "├──────────────┼─────────────┤\n",
      "│ disappointed │        9364 │\n",
      "├──────────────┼─────────────┤\n",
      "│ apologies    │       25380 │\n",
      "├──────────────┼─────────────┤\n",
      "│ enjoyed      │        5632 │\n",
      "├──────────────┼─────────────┤\n",
      "│ it           │        2009 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ,            │        1010 │\n",
      "├──────────────┼─────────────┤\n",
      "│ found        │        2179 │\n",
      "├──────────────┼─────────────┤\n",
      "│ com          │        4012 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ##ple        │       10814 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ##at         │        4017 │\n",
      "├──────────────┼─────────────┤\n",
      "│ al           │        2632 │\n",
      "├──────────────┼─────────────┤\n",
      "│ difficult    │        3697 │\n",
      "├──────────────┼─────────────┤\n",
      "│ watch        │        3422 │\n",
      "├──────────────┼─────────────┤\n",
      "│ got          │        2288 │\n",
      "├──────────────┼─────────────┤\n",
      "│ smiles       │        8451 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ,            │        1010 │\n",
      "├──────────────┼─────────────┤\n",
      "│ sure         │        2469 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ,            │        1010 │\n",
      "├──────────────┼─────────────┤\n",
      "│ majority     │        3484 │\n",
      "├──────────────┼─────────────┤\n",
      "│ funny        │        6057 │\n",
      "├──────────────┼─────────────┤\n",
      "│ came         │        2234 │\n",
      "├──────────────┼─────────────┤\n",
      "│ music        │        2189 │\n",
      "├──────────────┼─────────────┤\n",
      "│ videos       │        6876 │\n",
      "├──────────────┼─────────────┤\n",
      "│ got          │        2288 │\n",
      "├──────────────┼─────────────┤\n",
      "│ dvd          │        4966 │\n",
      "├──────────────┼─────────────┤\n",
      "│ rest         │        2717 │\n",
      "├──────────────┼─────────────┤\n",
      "│ basically    │       10468 │\n",
      "├──────────────┼─────────────┤\n",
      "│ fill         │        6039 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ##er         │        2121 │\n",
      "├──────────────┼─────────────┤\n",
      "│ could        │        2071 │\n",
      "├──────────────┼─────────────┤\n",
      "│ tell         │        2425 │\n",
      "├──────────────┼─────────────┤\n",
      "│ al           │        2632 │\n",
      "├──────────────┼─────────────┤\n",
      "│ greatest     │        4602 │\n",
      "├──────────────┼─────────────┤\n",
      "│ video        │        2678 │\n",
      "├──────────────┼─────────────┤\n",
      "│ achievement  │        6344 │\n",
      "├──────────────┼─────────────┤\n",
      "│ honor        │        3932 │\n",
      "├──────────────┼─────────────┤\n",
      "│ goes         │        3632 │\n",
      "├──────────────┼─────────────┤\n",
      "│ uhf          │       20131 │\n",
      "├──────────────┼─────────────┤\n",
      "│ honestly     │        9826 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ,            │        1010 │\n",
      "├──────────────┼─────────────┤\n",
      "│ doubt        │        4797 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ever         │        2412 │\n",
      "├──────────────┼─────────────┤\n",
      "│ make         │        2191 │\n",
      "├──────────────┼─────────────┤\n",
      "│ jump         │        5376 │\n",
      "├──────────────┼─────────────┤\n",
      "│ dvd          │        4966 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ,            │        1010 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ultra        │       11087 │\n",
      "├──────────────┼─────────────┤\n",
      "│ hardcore     │       13076 │\n",
      "├──────────────┼─────────────┤\n",
      "│ al           │        2632 │\n",
      "├──────────────┼─────────────┤\n",
      "│ fan          │        5470 │\n",
      "├──────────────┼─────────────┤\n",
      "│ everything   │        2673 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ,            │        1010 │\n",
      "├──────────────┼─────────────┤\n",
      "│ buy          │        4965 │\n",
      "├──────────────┼─────────────┤\n",
      "│ tape         │        6823 │\n",
      "├──────────────┼─────────────┤\n",
      "│ e            │        1041 │\n",
      "├──────────────┼─────────────┤\n",
      "│ ##bay        │       15907 │\n",
      "├──────────────┼─────────────┤\n",
      "│ pay          │        3477 │\n",
      "├──────────────┼─────────────┤\n",
      "│ much         │        2172 │\n",
      "╘══════════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "train_data['text'] = train_data['text'].apply(lambda x: clean_text(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x: clean_text(x))\n",
    "index = 0\n",
    "reviews = train_data.text.values\n",
    "abs = train_data.sentiment.values\n",
    "\n",
    "labels = []\n",
    "for i in abs:\n",
    "    if i == 'pos':\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "table = np.array([tokenizer.tokenize(reviews[index]), \n",
    "                    tokenizer.convert_tokens_to_ids(tokenizer.tokenize(reviews[index]))]).T\n",
    "print(tabulate(table,headers = ['Tokens', 'Token IDs'],tablefmt = 'fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a02f21bdc2644db6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:08.391506Z",
     "start_time": "2025-05-20T16:20:04.424200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  512\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in reviews:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True, max_length=tokenizer.model_max_length, truncation=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "26b130f81d4653",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:13.153773Z",
     "start_time": "2025-05-20T16:20:08.392512Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loris\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2700: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m input_ids = torch.cat(input_ids, dim=\u001b[32m0\u001b[39m)\n\u001b[32m     31\u001b[39m attention_masks = torch.cat(attention_masks, dim=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m labels = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every tweet...\n",
    "for sent in reviews:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_len,           # Pad & truncate all sentences.\n",
    "                        truncation=True,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb7bbeae04a353",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:13.154778Z",
     "start_time": "2025-05-20T16:20:13.154778Z"
    }
   },
   "outputs": [],
   "source": [
    "i=random.randint(0,len(reviews)-1)\n",
    "print('Original: ', reviews[i])\n",
    "print('Input IDs:', input_ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a6fbf47547cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rand_sentence_encoding():\n",
    "  '''Displays tokens, token IDs and attention mask of a random text sample'''\n",
    "  index = random.randint(0, len(reviews) - 1)\n",
    "  tokens = tokenizer.tokenize(tokenizer.decode(input_ids[index]))\n",
    "  token_ids = [i.numpy() for i in input_ids[index]]\n",
    "  attention = [i.numpy() for i in attention_masks[index]]\n",
    "\n",
    "  table = np.array([tokens, token_ids, attention]).T\n",
    "  print(reviews[index])\n",
    "  print(tabulate(table, \n",
    "                 headers = ['Tokens', 'Token IDs', 'Attention Mask'],\n",
    "                 tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996b9040c3e47ae0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:13.156780Z",
     "start_time": "2025-05-20T16:20:13.156780Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.8 * len(dataset))\n",
    "#val_size = int(0.2 * len(dataset))\n",
    "val_size = len(dataset)  - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd9b3b38338607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183150f57435747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_tp(preds, labels):\n",
    "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
    "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fp(preds, labels):\n",
    "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
    "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_tn(preds, labels):\n",
    "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
    "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fn(preds, labels):\n",
    "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
    "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_metrics(preds, labels):\n",
    "  '''\n",
    "  Returns the following metrics:\n",
    "    - accuracy    = (TP + TN) / N\n",
    "    - precision   = TP / (TP + FP)\n",
    "    - recall      = TP / (TP + FN)\n",
    "    - specificity = TN / (TN + FP)\n",
    "  '''\n",
    "  preds = np.argmax(preds, axis = 1).flatten()\n",
    "  labels = labels.flatten()\n",
    "  tp = b_tp(preds, labels)\n",
    "  tn = b_tn(preds, labels)\n",
    "  fp = b_fp(preds, labels)\n",
    "  fn = b_fn(preds, labels)\n",
    "  b_accuracy = (tp + tn) / len(labels)\n",
    "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
    "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
    "  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
    "  return b_accuracy, b_precision, b_recall, b_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335bd2004bc9d617",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:13.161779Z",
     "start_time": "2025-05-20T16:20:13.160780Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# if device == \"cuda:0\":\n",
    "# # Tell pytorch to run this model on the GPU.\n",
    "#     model = model.cuda()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c57acd68bf57f34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:13.162780Z",
     "start_time": "2025-05-20T16:20:13.162780Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd753f9887878a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0c26b46a67c0ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:13.164779Z",
     "start_time": "2025-05-20T16:20:13.164779Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e3678f2a79e94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:13.165779Z",
     "start_time": "2025-05-20T16:20:13.165779Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dcb0a1f97007d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:13.166779Z",
     "start_time": "2025-05-20T16:20:13.166779Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "\n",
    "for _ in trange(epochs, desc = 'Epoch'):\n",
    "    \n",
    "    # ========== Training ==========\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        train_output = model(b_input_ids, \n",
    "                             token_type_ids = None, \n",
    "                             attention_mask = b_input_mask, \n",
    "                             labels = b_labels)\n",
    "        # Backward pass\n",
    "        train_output.loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += train_output.loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    # ========== Validation ==========\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    val_accuracy = []\n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "    val_specificity = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "          # Forward pass\n",
    "          eval_output = model(b_input_ids, \n",
    "                              token_type_ids = None, \n",
    "                              attention_mask = b_input_mask)\n",
    "        logits = eval_output.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Calculate validation metrics\n",
    "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
    "        val_accuracy.append(b_accuracy)\n",
    "        # Update precision only when (tp + fp) !=0; ignore nan\n",
    "        if b_precision != 'nan': val_precision.append(b_precision)\n",
    "        # Update recall only when (tp + fn) !=0; ignore nan\n",
    "        if b_recall != 'nan': val_recall.append(b_recall)\n",
    "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
    "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
    "\n",
    "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
    "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
    "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
    "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
    "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8da8fb2db8dd98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:13.167779Z",
     "start_time": "2025-05-20T16:20:13.167779Z"
    }
   },
   "outputs": [],
   "source": [
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "test_tweets=['WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.']\n",
    "for tweet in test_tweets:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tweet,                     \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = max_len,         \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )\n",
    "    test_input_ids.append(encoded_dict['input_ids'])\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f5fb3ef617a45a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:13.168779Z",
     "start_time": "2025-05-20T16:20:13.168779Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for batch in test_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        with torch.no_grad():        \n",
    "            output= model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask)\n",
    "            logits = output.logits\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "            \n",
    "            predictions.extend(list(pred_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f8e5c1af016d09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:20:13.169779Z",
     "start_time": "2025-05-20T16:20:13.168779Z"
    }
   },
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame()\n",
    "df_output['tweets']=test_tweets\n",
    "df_output['label'] =predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d56afad0f182c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-20T16:20:13.169779Z"
    }
   },
   "outputs": [],
   "source": [
    "df_output.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
