{
 "cells": [
  {
   "cell_type": "code",
   "id": "12546e58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:19:26.875682Z",
     "start_time": "2025-07-07T13:19:26.871743Z"
    }
   },
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import re\n",
    "import demoji\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt_tab') #tokenizzazione\n",
    "nltk.download('stopwords') # stop words\n",
    "nltk.download('wordnet') # lemmatizzazione\n",
    "demoji.download_codes() # dizionario delle emoji"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\loris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\loris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\loris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\loris\\AppData\\Local\\Temp\\ipykernel_20012\\2824634240.py:15: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes() # dizionario delle emoji\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "55939328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:19:26.915395Z",
     "start_time": "2025-07-07T13:19:26.889436Z"
    }
   },
   "source": [
    "zip_path = 'redditComments.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_path) as z:\n",
    "    with z.open('ruddit_comments_score.csv') as csv:\n",
    "        df = pd.read_csv(csv)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (5966, 3)\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "f795f608",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:19:26.920564Z",
     "start_time": "2025-07-07T13:19:26.916401Z"
    }
   },
   "source": [
    "df = df.drop(columns='comment_id')\n",
    "print(df.columns)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['body', 'score'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "16c58bbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:19:26.925469Z",
     "start_time": "2025-07-07T13:19:26.921569Z"
    }
   },
   "source": [
    "sexual_words = [\n",
    "    \"sex\", \"porn\", \"xxx\", \"nude\", \"explicit\", \"18+\", \"nsfw\", \"hardcore\",\n",
    "    \"naked\", \"erotic\", \"fetish\", \"fuck\", \"bitch\", \"boobs\",\n",
    "    \"dick\", \"pussy\", \"cock\", \"slut\", \"milf\", \"cum\", \"orgasm\",\n",
    "    \"blowjob\", \"handjob\", \"deepthroat\", \"threesome\", \"gangbang\",\n",
    "    \"stripper\", \"hooker\", \"escort\", \"camgirl\", \"sugar daddy\",\n",
    "    \"bondage\", \"bdsm\", \"dominatrix\", \"kinky\", \"lick\", \"moan\",\n",
    "    \"anal\", \"vibrator\", \"masturbation\", \"climax\", \"sex tape\",\n",
    "    \"playboy\", \"sensual\", \"seduce\", \"twerking\", \"squirting\",\n",
    "    \"penetration\", \"foreplay\", \"erogenous\", \"nipple\", \"lap dance\",\n",
    "    \"thong\", \"lingerie\", \"panties\", \"semen\", \"horny\", \"aroused\",\n",
    "    \"provocative\", \"breasts\", \"vagina\", \"penis\", \"buttocks\",\n",
    "    \"oral sex\", \"oral action\", \"attractive\", \"seduction\",\n",
    "    \"softcore\", \"hardcore\", \"deepfake\", \"taboo\", \"cunnilingus\",\n",
    "    \"dominance\", \"submission\", \"degrading\", \"wet dream\",\n",
    "    \"nude selfies\", \"sugar baby\", \"sexual fantasy\",\n",
    "]\n",
    "\n",
    "violence_words = [\n",
    "    \"murder\", \"kill\", \"slaughter\", \"torture\", \"execute\", \"decapitate\",\n",
    "    \"terrorist\", \"bomb\", \"massacre\", \"shoot\", \"gun\", \"rifle\",\n",
    "    \"stabbing\", \"lynch\", \"assassinate\", \"bloodbath\", \"beheading\",\n",
    "    \"strangle\", \"rape\", \"abuse\", \"assault\", \"harassment\",\n",
    "    \"pedophile\", \"molestation\", \"kidnap\", \"hostage\", \"extortion\",\n",
    "    \"death threat\", \"violence\", \"riot\", \"fight\", \"attack\",\n",
    "    \"mugging\", \"aggression\", \"choke\", \"punch\", \"stab\", \"burn\",\n",
    "    \"hang\", \"drown\", \"brutal\", \"mutilation\", \"genocide\",\n",
    "    \"suffocate\", \"savage\", \"blood\", \"gore\", \"brutality\",\n",
    "    \"war crime\", \"child abuse\", \"animal abuse\", \"executed\",\n",
    "    \"tied up\", \"psychopath\", \"serial killer\", \"homicide\",\n",
    "    \"strangulation\", \"cruel\", \"sadistic\", \"deadly\", \"hatred\",\n",
    "    \"threat\", \"beaten\", \"burned alive\", \"gunned down\",\n",
    "    \"assaulted\", \"violated\", \"warzone\", \"dismember\",\n",
    "    \"disfigure\", \"tortured\", \"violent\", \"corpses\"\n",
    "]\n"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:19:30.172995Z",
     "start_time": "2025-07-07T13:19:26.926474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Rimozione dei commenti nulli\n",
    "df = df.dropna(subset=['body'])\n",
    "df.drop(df[df['body'].str.startswith('[')].index, inplace=True)\n",
    "\n",
    "words_to_count = violence_words + sexual_words\n",
    "comment_count = df[\"body\"].astype(str).apply(lambda comment: any(re.search(rf\"\\b{re.escape(word)}\\b\", comment, re.IGNORECASE) for word in words_to_count)).sum()\n",
    "print(f\"Numero di commenti contenenti almeno una parola della lista: {comment_count}\")"
   ],
   "id": "df9de9e049b8f426",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di commenti contenenti almeno una parola della lista: 1054\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:20:14.492780Z",
     "start_time": "2025-07-07T13:19:30.174398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dict1 = {\n",
    "    \"don't\": \"do not\", \"doesn't\": \"does not\", \"didn't\": \"did not\",\n",
    "    \"won't\": \"will not\", \"can't\": \"cannot\", \"i'm\": \"i am\", \"you're\": \"you are\",\n",
    "    \"it's\": \"it is\", \"they're\": \"they are\", \"we're\": \"we are\", \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\", \"haven't\": \"have not\", \"hasn't\": \"has not\",\n",
    "    \"wasn't\": \"was not\", \"weren't\": \"were not\", \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\", \"couldn't\": \"could not\", \"mustn't\": \"must not\"\n",
    "}\n",
    "\n",
    "dict2 = {\n",
    "    \"nude\": \"naked\", \"xxx\": \"porn\", \"tits\": \"breasts\", \"dick\": \"penis\",\n",
    "    \"pussy\": \"vagina\", \"bj\": \"blowjob\", \"cum\": \"semen\", \"hornyk\": \"aroused\",\n",
    "    \"milf\": \"attractive older woman\", \"nsfw\": \"not safe for work\",\n",
    "    \"thirsty\": \"sexually desperate\", \"ass\": \"buttocks\", \"booty\": \"buttocks\",\n",
    "    \"deepthroat\": \"oral sex\", \"suck\": \"oral action\", \"sexy\": \"attractive\"\n",
    "}\n",
    "\n",
    "def expand_contractions_and_slang(text):\n",
    "    #applicazione dei dizionari\n",
    "    for key, value in {**dict1, **dict2}.items():\n",
    "        text = re.sub(r\"\\b\" + key + r\"\\b\", value, text)\n",
    "    return text\n",
    "\n",
    "def correct_spelling(text):\n",
    "    #correzione degli errori ortografici\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "def segment_sentences(text):\n",
    "    # divisione in frasi\n",
    "    return sent_tokenize(text) if isinstance(text, str) else []\n",
    "\n",
    "def replace_emojis_and_ascii(text):\n",
    "    text = demoji.replace_with_desc(text)\n",
    "\n",
    "    # rimozione elementi non validi\n",
    "    text = re.sub(r'(\\*\\*|\\*\\w+\\*|\\W+\\*{2,})', '', text)\n",
    "    text = re.sub(r'(\\bxx+\\b)', 'explicit', text) \n",
    "    return text\n",
    "\n",
    "important_stopwords = {\"not\", \"no\", \"nor\", \"n't\", \"never\", \"hardly\", \"barely\", \"scarcely\",\n",
    "                       \"very\", \"really\", \"so\", \"extremely\", \"super\", \"terribly\", \"horribly\",\n",
    "                       \"awfully\", \"slightly\", \"somewhat\", \"but\", \"however\", \"although\",\n",
    "                       \"though\", \"yet\", \"i\", \"we\", \"you\", \"he\", \"she\", \"they\", \"my\", \"your\",\n",
    "                       \"why\", \"how\", \"what\"}\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\")) - important_stopwords\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Esegue il preprocessing completo di un testo per adult content filtering.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = expand_contractions_and_slang(text)\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "        text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "        text = replace_emojis_and_ascii(text)\n",
    "        text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "        tokenizer = TweetTokenizer()\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        text = correct_spelling(\" \".join(tokens))\n",
    "\n",
    "        return text\n",
    "    return \"\"\n",
    "df['Segmented_Comments'] = df['body'].apply(segment_sentences)\n",
    "df['Processed_Comment'] = df['body'].apply(preprocess_text)\n",
    "\n",
    "df.head()"
   ],
   "id": "5801330ef0c8ac98",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[46]\u001B[39m\u001B[32m, line 68\u001B[39m\n\u001B[32m     66\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     67\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33mSegmented_Comments\u001B[39m\u001B[33m'\u001B[39m] = df[\u001B[33m'\u001B[39m\u001B[33mbody\u001B[39m\u001B[33m'\u001B[39m].apply(segment_sentences)\n\u001B[32m---> \u001B[39m\u001B[32m68\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33mProcessed_Comment\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mbody\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreprocess_text\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     70\u001B[39m df.head()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001B[39m, in \u001B[36mSeries.apply\u001B[39m\u001B[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[39m\n\u001B[32m   4789\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mapply\u001B[39m(\n\u001B[32m   4790\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   4791\u001B[39m     func: AggFuncType,\n\u001B[32m   (...)\u001B[39m\u001B[32m   4796\u001B[39m     **kwargs,\n\u001B[32m   4797\u001B[39m ) -> DataFrame | Series:\n\u001B[32m   4798\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   4799\u001B[39m \u001B[33;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[32m   4800\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   4915\u001B[39m \u001B[33;03m    dtype: float64\u001B[39;00m\n\u001B[32m   4916\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m   4917\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4918\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   4919\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4920\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4921\u001B[39m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[43m=\u001B[49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4922\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4923\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m-> \u001B[39m\u001B[32m4924\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001B[39m, in \u001B[36mSeriesApply.apply\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1424\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.apply_compat()\n\u001B[32m   1426\u001B[39m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1427\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001B[39m, in \u001B[36mSeriesApply.apply_standard\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1501\u001B[39m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[32m   1502\u001B[39m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[32m   1503\u001B[39m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[32m   1504\u001B[39m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[32m   1505\u001B[39m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[32m   1506\u001B[39m action = \u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj.dtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1507\u001B[39m mapped = \u001B[43mobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1508\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m=\u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[32m   1509\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1511\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[32m0\u001B[39m], ABCSeries):\n\u001B[32m   1512\u001B[39m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[32m   1513\u001B[39m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[32m   1514\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj._constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index=obj.index)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001B[39m, in \u001B[36mIndexOpsMixin._map_values\u001B[39m\u001B[34m(self, mapper, na_action, convert)\u001B[39m\n\u001B[32m    918\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[32m    919\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m arr.map(mapper, na_action=na_action)\n\u001B[32m--> \u001B[39m\u001B[32m921\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m=\u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001B[39m, in \u001B[36mmap_array\u001B[39m\u001B[34m(arr, mapper, na_action, convert)\u001B[39m\n\u001B[32m   1741\u001B[39m values = arr.astype(\u001B[38;5;28mobject\u001B[39m, copy=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m   1742\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1743\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1745\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m lib.map_infer_mask(\n\u001B[32m   1746\u001B[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001B[32m   1747\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mlib.pyx:2972\u001B[39m, in \u001B[36mpandas._libs.lib.map_infer\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[46]\u001B[39m\u001B[32m, line 63\u001B[39m, in \u001B[36mpreprocess_text\u001B[39m\u001B[34m(text)\u001B[39m\n\u001B[32m     61\u001B[39m     lemmatizer = WordNetLemmatizer()\n\u001B[32m     62\u001B[39m     tokens = [lemmatizer.lemmatize(word) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m tokens]\n\u001B[32m---> \u001B[39m\u001B[32m63\u001B[39m     text = \u001B[43mcorrect_spelling\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     65\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m text\n\u001B[32m     66\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[46]\u001B[39m\u001B[32m, line 26\u001B[39m, in \u001B[36mcorrect_spelling\u001B[39m\u001B[34m(text)\u001B[39m\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcorrect_spelling\u001B[39m(text):\n\u001B[32m     25\u001B[39m     \u001B[38;5;66;03m#correzione degli errori ortografici\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(\u001B[43mTextBlob\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcorrect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\blob.py:555\u001B[39m, in \u001B[36mBaseBlob.correct\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    553\u001B[39m tokens = nltk.tokenize.regexp_tokenize(\u001B[38;5;28mself\u001B[39m.raw, \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw+|[^\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw\u001B[39m\u001B[33m\\\u001B[39m\u001B[33ms]|\u001B[39m\u001B[33m\\\u001B[39m\u001B[33ms\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    554\u001B[39m corrected = (Word(w).correct() \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m tokens)\n\u001B[32m--> \u001B[39m\u001B[32m555\u001B[39m ret = \u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorrected\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    556\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m(ret)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\blob.py:554\u001B[39m, in \u001B[36m<genexpr>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m    552\u001B[39m \u001B[38;5;66;03m# regex matches: word or punctuation or whitespace\u001B[39;00m\n\u001B[32m    553\u001B[39m tokens = nltk.tokenize.regexp_tokenize(\u001B[38;5;28mself\u001B[39m.raw, \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw+|[^\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw\u001B[39m\u001B[33m\\\u001B[39m\u001B[33ms]|\u001B[39m\u001B[33m\\\u001B[39m\u001B[33ms\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m554\u001B[39m corrected = (\u001B[43mWord\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcorrect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m tokens)\n\u001B[32m    555\u001B[39m ret = \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m.join(corrected)\n\u001B[32m    556\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m(ret)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\blob.py:115\u001B[39m, in \u001B[36mWord.correct\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    109\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcorrect\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    110\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Correct the spelling of the word. Returns the word with the highest\u001B[39;00m\n\u001B[32m    111\u001B[39m \u001B[33;03m    confidence using the spelling corrector.\u001B[39;00m\n\u001B[32m    112\u001B[39m \n\u001B[32m    113\u001B[39m \u001B[33;03m    .. versionadded:: 0.6.0\u001B[39;00m\n\u001B[32m    114\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m115\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m Word(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mspellcheck\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\blob.py:107\u001B[39m, in \u001B[36mWord.spellcheck\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     98\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mspellcheck\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m     99\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Return a list of (word, confidence) tuples of spelling corrections.\u001B[39;00m\n\u001B[32m    100\u001B[39m \n\u001B[32m    101\u001B[39m \u001B[33;03m    Based on: Peter Norvig, \"How to Write a Spelling Corrector\"\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    105\u001B[39m \u001B[33;03m    .. versionadded:: 0.6.0\u001B[39;00m\n\u001B[32m    106\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m107\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msuggest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstring\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\en\\__init__.py:118\u001B[39m, in \u001B[36msuggest\u001B[39m\u001B[34m(w)\u001B[39m\n\u001B[32m    116\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34msuggest\u001B[39m(w):\n\u001B[32m    117\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Returns a list of (word, confidence)-tuples of spelling corrections.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m118\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mspelling\u001B[49m\u001B[43m.\u001B[49m\u001B[43msuggest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\_text.py:1692\u001B[39m, in \u001B[36mSpelling.suggest\u001B[39m\u001B[34m(self, w)\u001B[39m\n\u001B[32m   1687\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m w.replace(\u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m).isdigit():\n\u001B[32m   1688\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [(w, \u001B[32m1.0\u001B[39m)]  \u001B[38;5;66;03m# 1.5\u001B[39;00m\n\u001B[32m   1689\u001B[39m candidates = (\n\u001B[32m   1690\u001B[39m     \u001B[38;5;28mself\u001B[39m._known([w])\n\u001B[32m   1691\u001B[39m     \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._known(\u001B[38;5;28mself\u001B[39m._edit1(w))\n\u001B[32m-> \u001B[39m\u001B[32m1692\u001B[39m     \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._known(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_edit2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m   1693\u001B[39m     \u001B[38;5;129;01mor\u001B[39;00m [w]\n\u001B[32m   1694\u001B[39m )\n\u001B[32m   1695\u001B[39m candidates = [(\u001B[38;5;28mself\u001B[39m.get(c, \u001B[32m0.0\u001B[39m), c) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m candidates]\n\u001B[32m   1696\u001B[39m s = \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;28msum\u001B[39m(p \u001B[38;5;28;01mfor\u001B[39;00m p, word \u001B[38;5;129;01min\u001B[39;00m candidates) \u001B[38;5;129;01mor\u001B[39;00m \u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\_text.py:1667\u001B[39m, in \u001B[36mSpelling._edit2\u001B[39m\u001B[34m(self, w)\u001B[39m\n\u001B[32m   1664\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Returns a set of words with edit distance 2 from the given word\"\"\"\u001B[39;00m\n\u001B[32m   1665\u001B[39m \u001B[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001B[39;00m\n\u001B[32m   1666\u001B[39m \u001B[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1667\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mset\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43me2\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43me1\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_edit1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43me2\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_edit1\u001B[49m\u001B[43m(\u001B[49m\u001B[43me1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43me2\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\_text.py:1667\u001B[39m, in \u001B[36m<genexpr>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m   1664\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Returns a set of words with edit distance 2 from the given word\"\"\"\u001B[39;00m\n\u001B[32m   1665\u001B[39m \u001B[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001B[39;00m\n\u001B[32m   1666\u001B[39m \u001B[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1667\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mset\u001B[39m(e2 \u001B[38;5;28;01mfor\u001B[39;00m e1 \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._edit1(w) \u001B[38;5;28;01mfor\u001B[39;00m e2 \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._edit1(e1) \u001B[38;5;28;01mif\u001B[39;00m \u001B[43me2\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\_text.py:104\u001B[39m, in \u001B[36mlazydict.__contains__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m    103\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__contains__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args):\n\u001B[32m--> \u001B[39m\u001B[32m104\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_lazy\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m__contains__\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "words_to_count = sexual_words + violence_words\n",
    "\n",
    "def classify_comment(comment):\n",
    "    if any(re.search(rf\"\\b{re.escape(word)}\\b\", str(comment), re.IGNORECASE) for word in words_to_count):\n",
    "        return 1  # Commenti classificati per adulti\n",
    "    else:\n",
    "        return 0  # Commenti senza restrizioni\n",
    "\n",
    "df['Content_Label'] = df['Processed_Comment'].apply(classify_comment)\n",
    "\n",
    "print(df['Content_Label'].value_counts())\n",
    "df.info()"
   ],
   "id": "102a62ff238680e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pandas as pd\n",
    "\n",
    "class_counts = df['Content_Label'].value_counts()\n",
    "total_adult = class_counts[1]\n",
    "total_safe = class_counts[0] \n",
    "target_safe = int(total_adult * (60 / 40))\n",
    "\n",
    "if total_safe > target_safe:\n",
    "    undersampler = RandomUnderSampler(sampling_strategy={0: target_safe}, random_state=42)\n",
    "    x_balanced, y_balanced = undersampler.fit_resample(df, df['Content_Label'])\n",
    "\n",
    "    print(\"\\n🔹 Distribuzione delle classi:\\n\", pd.Series(y_balanced).value_counts())\n",
    "\n",
    "    df_balanced = pd.DataFrame(x_balanced)\n",
    "else:\n",
    "    df_balanced = df  # Se già bilanciato, usa il dataset originale"
   ],
   "id": "85c56689d3de7de9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"🔹 Using device: {device}\")\n",
    "\n",
    "print(\"🔹 Distribuzione delle classi dopo il bilanciamento:\\n\", df_balanced['Content_Label'].value_counts())\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df_balanced['Processed_Comment'].tolist(), df_balanced['Content_Label'].tolist(),\n",
    "    test_size=0.2, stratify=df_balanced['Content_Label'], random_state=42\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding='max_length', max_length=70, return_tensors='pt')\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding='max_length', max_length=70, return_tensors='pt')\n",
    "\n",
    "class ContentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = ContentDataset(train_encodings, train_labels)\n",
    "test_dataset = ContentDataset(test_encodings, test_labels)\n",
    "\n",
    "config = BertConfig.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2,  # 🔹 Classificazione binaria\n",
    "    hidden_dropout_prob=0.3,\n",
    "    attention_probs_dropout_prob=0.3\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "class_counts = df_balanced['Content_Label'].value_counts().sort_index()\n",
    "total_samples = len(df_balanced)\n",
    "class_weights = {i: total_samples / class_counts[i] for i in class_counts.index}\n",
    "class_weights_tensor = torch.tensor(list(class_weights.values()), dtype=torch.float).to(device)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=5):\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    results = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for batch in loop:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_description(f'Epoch {epoch + 1}')\n",
    "            loop.set_postfix(loss=loss.item(), acc=correct / total)\n",
    "\n",
    "        train_accuracy = correct / total\n",
    "        avg_training_loss = total_loss / len(train_loader)\n",
    "\n",
    "        val_loss, val_accuracy = evaluate_model(model, val_loader)\n",
    "\n",
    "        train_losses.append(avg_training_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f\"\\n🔹 Epoch {epoch + 1}: Training Loss: {avg_training_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"🔹 Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        results.append({\n",
    "            \"Epoch\": epoch + 1,\n",
    "            \"Training Loss\": round(avg_training_loss, 4),\n",
    "            \"Training Accuracy\": round(train_accuracy, 4),\n",
    "            \"Validation Loss\": round(val_loss, 4),\n",
    "            \"Validation Accuracy\": round(val_accuracy, 4)\n",
    "        })\n",
    "\n",
    "    plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df)\n",
    "    return results_df\n",
    "\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(val_loader), correct / total\n",
    "\n",
    "\n",
    "def plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training & Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training & Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "results_df = train_model(model, train_loader, val_loader, epochs=5)"
   ],
   "id": "b775ea74a25e2b67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def plot_confusion_matrix(model, val_loader, class_names=['safe', 'adult']):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n🔹 Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "plot_confusion_matrix(model, val_loader)"
   ],
   "id": "a5835287dadf8361"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
