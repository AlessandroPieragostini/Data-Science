{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:30:56.709510Z",
     "start_time": "2025-07-07T13:30:56.703233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import demoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "nltk.download('punkt_tab') #tokenizzazione\n",
    "nltk.download('stopwords') # stop words\n",
    "nltk.download('wordnet') # lemmatizzazione\n",
    "demoji.download_codes() # dizionario delle emoji"
   ],
   "id": "fd94d3dad8657b61",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\loris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\loris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\loris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\loris\\AppData\\Local\\Temp\\ipykernel_1568\\3196436983.py:16: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes() # dizionario delle emoji\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:30:56.753266Z",
     "start_time": "2025-07-07T13:30:56.711778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "zip_path = 'redditComments.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_path) as z:\n",
    "    with z.open('ruddit_comments_score.csv') as csv:\n",
    "        df = pd.read_csv(csv)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)"
   ],
   "id": "ffa5e8e661141d0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (5966, 3)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:30:56.758568Z",
     "start_time": "2025-07-07T13:30:56.754271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = df.drop(columns='comment_id')\n",
    "print(df.columns)"
   ],
   "id": "770e7ad941b8dc54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['body', 'score'], dtype='object')\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:30:56.769648Z",
     "start_time": "2025-07-07T13:30:56.759575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Rimozione dei commenti nulli\n",
    "df = df.dropna(subset=['body'])\n",
    "df.drop(df[df['body'].str.startswith('[')].index, inplace=True)\n"
   ],
   "id": "e371ced79894d0af",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:31:13.362892Z",
     "start_time": "2025-07-07T13:30:56.770653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dict1 = {\n",
    "    \"don't\": \"do not\", \"doesn't\": \"does not\", \"didn't\": \"did not\",\n",
    "    \"won't\": \"will not\", \"can't\": \"cannot\", \"i'm\": \"i am\", \"you're\": \"you are\",\n",
    "    \"it's\": \"it is\", \"they're\": \"they are\", \"we're\": \"we are\", \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\", \"haven't\": \"have not\", \"hasn't\": \"has not\",\n",
    "    \"wasn't\": \"was not\", \"weren't\": \"were not\", \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\", \"couldn't\": \"could not\", \"mustn't\": \"must not\"\n",
    "}\n",
    "\n",
    "dict2 = {\n",
    "    \"nude\": \"naked\", \"xxx\": \"porn\", \"tits\": \"breasts\", \"dick\": \"penis\",\n",
    "    \"pussy\": \"vagina\", \"bj\": \"blowjob\", \"cum\": \"semen\", \"hornyk\": \"aroused\",\n",
    "    \"milf\": \"attractive older woman\", \"nsfw\": \"not safe for work\",\n",
    "    \"thirsty\": \"sexually desperate\", \"ass\": \"buttocks\", \"booty\": \"buttocks\",\n",
    "    \"deepthroat\": \"oral sex\", \"suck\": \"oral action\", \"sexy\": \"attractive\"\n",
    "}\n",
    "\n",
    "def expand_contractions_and_slang(text):\n",
    "    #applicazione dei dizionari\n",
    "    for key, value in {**dict1, **dict2}.items():\n",
    "        text = re.sub(r\"\\b\" + key + r\"\\b\", value, text)\n",
    "    return text\n",
    "\n",
    "def correct_spelling(text):\n",
    "    #correzione degli errori ortografici\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "def segment_sentences(text):\n",
    "    # divisione in frasi\n",
    "    return sent_tokenize(text) if isinstance(text, str) else []\n",
    "\n",
    "def replace_emojis_and_ascii(text):\n",
    "    text = demoji.replace_with_desc(text)\n",
    "\n",
    "    # rimozione elementi non validi\n",
    "    text = re.sub(r'(\\*\\*|\\*\\w+\\*|\\W+\\*{2,})', '', text)\n",
    "    text = re.sub(r'(\\bxx+\\b)', 'explicit', text) \n",
    "    return text\n",
    "\n",
    "important_stopwords = {\"not\", \"no\", \"nor\", \"n't\", \"never\", \"hardly\", \"barely\", \"scarcely\",\n",
    "                       \"very\", \"really\", \"so\", \"extremely\", \"super\", \"terribly\", \"horribly\",\n",
    "                       \"awfully\", \"slightly\", \"somewhat\", \"but\", \"however\", \"although\",\n",
    "                       \"though\", \"yet\", \"i\", \"we\", \"you\", \"he\", \"she\", \"they\", \"my\", \"your\",\n",
    "                       \"why\", \"how\", \"what\"}\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\")) - important_stopwords\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Esegue il preprocessing completo di un testo per adult content filtering.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = expand_contractions_and_slang(text)\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "        text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "        text = replace_emojis_and_ascii(text)\n",
    "        text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "        tokenizer = TweetTokenizer()\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        text = correct_spelling(\" \".join(tokens))\n",
    "\n",
    "        return text\n",
    "    return \"\"\n",
    "df['Segmented_Comments'] = df['body'].apply(segment_sentences)\n",
    "df['Processed_Comment'] = df['body'].apply(preprocess_text)\n",
    "\n",
    "df.head()\n",
    "# Salvataggio del dataset preprocessato in formato Pickle\n",
    "df.to_pickle(\"datasets/Preprocessed_YoutubeComments_ACF.pkl\")\n",
    "print(\"Dataset salvato in formato Pickle!\")"
   ],
   "id": "a64b59ed135b897b",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 68\u001B[39m\n\u001B[32m     66\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     67\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33mSegmented_Comments\u001B[39m\u001B[33m'\u001B[39m] = df[\u001B[33m'\u001B[39m\u001B[33mbody\u001B[39m\u001B[33m'\u001B[39m].apply(segment_sentences)\n\u001B[32m---> \u001B[39m\u001B[32m68\u001B[39m df[\u001B[33m'\u001B[39m\u001B[33mProcessed_Comment\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mbody\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreprocess_text\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     70\u001B[39m df.head()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001B[39m, in \u001B[36mSeries.apply\u001B[39m\u001B[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[39m\n\u001B[32m   4789\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mapply\u001B[39m(\n\u001B[32m   4790\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   4791\u001B[39m     func: AggFuncType,\n\u001B[32m   (...)\u001B[39m\u001B[32m   4796\u001B[39m     **kwargs,\n\u001B[32m   4797\u001B[39m ) -> DataFrame | Series:\n\u001B[32m   4798\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   4799\u001B[39m \u001B[33;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[32m   4800\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   4915\u001B[39m \u001B[33;03m    dtype: float64\u001B[39;00m\n\u001B[32m   4916\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m   4917\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   4918\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   4919\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4920\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4921\u001B[39m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[43m=\u001B[49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4922\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4923\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m-> \u001B[39m\u001B[32m4924\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001B[39m, in \u001B[36mSeriesApply.apply\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1424\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.apply_compat()\n\u001B[32m   1426\u001B[39m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1427\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001B[39m, in \u001B[36mSeriesApply.apply_standard\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1501\u001B[39m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[32m   1502\u001B[39m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[32m   1503\u001B[39m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[32m   1504\u001B[39m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[32m   1505\u001B[39m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[32m   1506\u001B[39m action = \u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj.dtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1507\u001B[39m mapped = \u001B[43mobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1508\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m=\u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[32m   1509\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1511\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[32m0\u001B[39m], ABCSeries):\n\u001B[32m   1512\u001B[39m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[32m   1513\u001B[39m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[32m   1514\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj._constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index=obj.index)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001B[39m, in \u001B[36mIndexOpsMixin._map_values\u001B[39m\u001B[34m(self, mapper, na_action, convert)\u001B[39m\n\u001B[32m    918\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[32m    919\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m arr.map(mapper, na_action=na_action)\n\u001B[32m--> \u001B[39m\u001B[32m921\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m=\u001B[49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001B[39m, in \u001B[36mmap_array\u001B[39m\u001B[34m(arr, mapper, na_action, convert)\u001B[39m\n\u001B[32m   1741\u001B[39m values = arr.astype(\u001B[38;5;28mobject\u001B[39m, copy=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m   1742\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1743\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1744\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1745\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m lib.map_infer_mask(\n\u001B[32m   1746\u001B[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001B[32m   1747\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mlib.pyx:2972\u001B[39m, in \u001B[36mpandas._libs.lib.map_infer\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 63\u001B[39m, in \u001B[36mpreprocess_text\u001B[39m\u001B[34m(text)\u001B[39m\n\u001B[32m     61\u001B[39m     lemmatizer = WordNetLemmatizer()\n\u001B[32m     62\u001B[39m     tokens = [lemmatizer.lemmatize(word) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m tokens]\n\u001B[32m---> \u001B[39m\u001B[32m63\u001B[39m     text = \u001B[43mcorrect_spelling\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     65\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m text\n\u001B[32m     66\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 26\u001B[39m, in \u001B[36mcorrect_spelling\u001B[39m\u001B[34m(text)\u001B[39m\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcorrect_spelling\u001B[39m(text):\n\u001B[32m     25\u001B[39m     \u001B[38;5;66;03m#correzione degli errori ortografici\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(\u001B[43mTextBlob\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcorrect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\blob.py:555\u001B[39m, in \u001B[36mBaseBlob.correct\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    553\u001B[39m tokens = nltk.tokenize.regexp_tokenize(\u001B[38;5;28mself\u001B[39m.raw, \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw+|[^\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw\u001B[39m\u001B[33m\\\u001B[39m\u001B[33ms]|\u001B[39m\u001B[33m\\\u001B[39m\u001B[33ms\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    554\u001B[39m corrected = (Word(w).correct() \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m tokens)\n\u001B[32m--> \u001B[39m\u001B[32m555\u001B[39m ret = \u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorrected\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    556\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m(ret)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\blob.py:554\u001B[39m, in \u001B[36m<genexpr>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m    552\u001B[39m \u001B[38;5;66;03m# regex matches: word or punctuation or whitespace\u001B[39;00m\n\u001B[32m    553\u001B[39m tokens = nltk.tokenize.regexp_tokenize(\u001B[38;5;28mself\u001B[39m.raw, \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw+|[^\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw\u001B[39m\u001B[33m\\\u001B[39m\u001B[33ms]|\u001B[39m\u001B[33m\\\u001B[39m\u001B[33ms\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m554\u001B[39m corrected = (\u001B[43mWord\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcorrect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m tokens)\n\u001B[32m    555\u001B[39m ret = \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m.join(corrected)\n\u001B[32m    556\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m(ret)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\blob.py:115\u001B[39m, in \u001B[36mWord.correct\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    109\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcorrect\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    110\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Correct the spelling of the word. Returns the word with the highest\u001B[39;00m\n\u001B[32m    111\u001B[39m \u001B[33;03m    confidence using the spelling corrector.\u001B[39;00m\n\u001B[32m    112\u001B[39m \n\u001B[32m    113\u001B[39m \u001B[33;03m    .. versionadded:: 0.6.0\u001B[39;00m\n\u001B[32m    114\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m115\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m Word(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mspellcheck\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\blob.py:107\u001B[39m, in \u001B[36mWord.spellcheck\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     98\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mspellcheck\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m     99\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Return a list of (word, confidence) tuples of spelling corrections.\u001B[39;00m\n\u001B[32m    100\u001B[39m \n\u001B[32m    101\u001B[39m \u001B[33;03m    Based on: Peter Norvig, \"How to Write a Spelling Corrector\"\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    105\u001B[39m \u001B[33;03m    .. versionadded:: 0.6.0\u001B[39;00m\n\u001B[32m    106\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m107\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msuggest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstring\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\en\\__init__.py:118\u001B[39m, in \u001B[36msuggest\u001B[39m\u001B[34m(w)\u001B[39m\n\u001B[32m    116\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34msuggest\u001B[39m(w):\n\u001B[32m    117\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Returns a list of (word, confidence)-tuples of spelling corrections.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m118\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mspelling\u001B[49m\u001B[43m.\u001B[49m\u001B[43msuggest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\_text.py:1692\u001B[39m, in \u001B[36mSpelling.suggest\u001B[39m\u001B[34m(self, w)\u001B[39m\n\u001B[32m   1687\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m w.replace(\u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m).isdigit():\n\u001B[32m   1688\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [(w, \u001B[32m1.0\u001B[39m)]  \u001B[38;5;66;03m# 1.5\u001B[39;00m\n\u001B[32m   1689\u001B[39m candidates = (\n\u001B[32m   1690\u001B[39m     \u001B[38;5;28mself\u001B[39m._known([w])\n\u001B[32m   1691\u001B[39m     \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._known(\u001B[38;5;28mself\u001B[39m._edit1(w))\n\u001B[32m-> \u001B[39m\u001B[32m1692\u001B[39m     \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._known(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_edit2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m   1693\u001B[39m     \u001B[38;5;129;01mor\u001B[39;00m [w]\n\u001B[32m   1694\u001B[39m )\n\u001B[32m   1695\u001B[39m candidates = [(\u001B[38;5;28mself\u001B[39m.get(c, \u001B[32m0.0\u001B[39m), c) \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m candidates]\n\u001B[32m   1696\u001B[39m s = \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;28msum\u001B[39m(p \u001B[38;5;28;01mfor\u001B[39;00m p, word \u001B[38;5;129;01min\u001B[39;00m candidates) \u001B[38;5;129;01mor\u001B[39;00m \u001B[32m1\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\_text.py:1667\u001B[39m, in \u001B[36mSpelling._edit2\u001B[39m\u001B[34m(self, w)\u001B[39m\n\u001B[32m   1664\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Returns a set of words with edit distance 2 from the given word\"\"\"\u001B[39;00m\n\u001B[32m   1665\u001B[39m \u001B[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001B[39;00m\n\u001B[32m   1666\u001B[39m \u001B[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1667\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mset\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43me2\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43me1\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_edit1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43me2\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_edit1\u001B[49m\u001B[43m(\u001B[49m\u001B[43me1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43me2\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\_text.py:1667\u001B[39m, in \u001B[36m<genexpr>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m   1664\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Returns a set of words with edit distance 2 from the given word\"\"\"\u001B[39;00m\n\u001B[32m   1665\u001B[39m \u001B[38;5;66;03m# Of all spelling errors, 99% is covered by edit distance 2.\u001B[39;00m\n\u001B[32m   1666\u001B[39m \u001B[38;5;66;03m# Only keep candidates that are actually known words (20% speedup).\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1667\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mset\u001B[39m(e2 \u001B[38;5;28;01mfor\u001B[39;00m e1 \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._edit1(w) \u001B[38;5;28;01mfor\u001B[39;00m e2 \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._edit1(e1) \u001B[38;5;28;01mif\u001B[39;00m \u001B[43me2\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\_text.py:104\u001B[39m, in \u001B[36mlazydict.__contains__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m    103\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__contains__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args):\n\u001B[32m--> \u001B[39m\u001B[32m104\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_lazy\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m__contains__\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\textblob\\_text.py:85\u001B[39m, in \u001B[36mlazydict._lazy\u001B[39m\u001B[34m(self, method, *args)\u001B[39m\n\u001B[32m     80\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mload\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m     81\u001B[39m     \u001B[38;5;66;03m# Must be overridden in a subclass.\u001B[39;00m\n\u001B[32m     82\u001B[39m     \u001B[38;5;66;03m# Must load data with dict.__setitem__(self, k, v) instead of lazydict[k] = v.\u001B[39;00m\n\u001B[32m     83\u001B[39m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m85\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_lazy\u001B[39m(\u001B[38;5;28mself\u001B[39m, method, *args):\n\u001B[32m     86\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"If the dictionary is empty, calls lazydict.load().\u001B[39;00m\n\u001B[32m     87\u001B[39m \u001B[33;03m    Replaces lazydict.method() with dict.method() and calls it.\u001B[39;00m\n\u001B[32m     88\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m     89\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mdict\u001B[39m.\u001B[34m__len__\u001B[39m(\u001B[38;5;28mself\u001B[39m) == \u001B[32m0\u001B[39m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Caricamento del dataset preprocessato da Pickle\n",
    "path_pickle = \"datasets/Preprocessed_YoutubeComments_ACF.pkl\"\n",
    "\n",
    "df = pd.read_pickle(path_pickle)\n",
    "df.head()\n",
    "df.info()"
   ],
   "id": "fea88a9c2ce2343"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:31:13.363400Z",
     "start_time": "2025-07-07T13:31:13.363400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_words = ' '.join([str(text) for text in df['Processed_Comment']])\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "98e5df359cfbb54c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:31:13.364405Z",
     "start_time": "2025-07-07T13:31:13.364405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity  # Polarità [-1, 1]\n",
    "\n",
    "# Applicare la funzione al dataset\n",
    "df['sentiment_score'] = df['Processed_Comment'].apply(get_sentiment)\n",
    "\n",
    "# Visualizzare i primi risultati\n",
    "print(df[['Processed_Comment', 'sentiment_score']].head())\n",
    "\n",
    "# Distribuzione dei punteggi di sentimento\n",
    "plt.hist(df['sentiment_score'], bins=20, edgecolor='black')\n",
    "plt.title('Distribuzione dei punteggi di sentimento')\n",
    "plt.xlabel('Sentiment score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "71dfb5f82c86d6ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Salvataggio del dataset preprocessato in formato Pickle\n",
    "df.to_pickle(\"datasets/Preprocessed_RedditComments.pkl\")\n",
    "print(\"Dataset salvato in formato Pickle!\")"
   ],
   "id": "e07bdb44844b6bfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "path_pickle = \"datasets/Preprocessed_RedditComments.pkl\"#\n",
    "df = pd.read_pickle(path_pickle)\n",
    "df.head()"
   ],
   "id": "9f6e9040083d57f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class_distribution = df['Sentiment'].value_counts()\n",
    "\n",
    "class_distribution\n",
    "\n",
    "# 📌 Stampa la distribuzione numerica\n",
    "print(\"🔹 Distribuzione delle classi nel dataset:\")\n",
    "print(class_distribution)\n",
    "\n",
    "# 📌 Visualizzazione della distribuzione con un grafico a barre\n",
    "plt.figure(figsize=(6, 4))\n",
    "class_distribution.plot(kind='bar', color=['salmon', 'lightblue', 'lightgreen'])\n",
    "plt.title(\"Distribuzione delle Classi di Sentiment\")\n",
    "plt.xlabel(\"Categoria di Sentiment\")\n",
    "plt.ylabel(\"Numero di Campioni\")\n",
    "\n",
    "# 📌 Aggiungere i numeri sopra le barre\n",
    "for index, value in enumerate(class_distribution):\n",
    "    plt.text(index, value + 5, str(value), ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ],
   "id": "1abe3636aa570824"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TF-IDF",
   "id": "4658573b760b3790"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 📌 Convertire il testo in TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=15000, ngram_range=(1,3))  # Include unigrammi, bigrammi e trigrammi\n",
    "X_tfidf = tfidf.fit_transform(df['Processed_Comment'])\n",
    "\n",
    "# 📌 Stampare la distribuzione delle classi PRIMA del bilanciamento\n",
    "print(\"🔹 Distribuzione classi PRIMA del bilanciamento:\")\n",
    "print(df['Sentiment'].value_counts())\n",
    "\n",
    "# 📌 1️⃣ **Downsampling della classe positiva**\n",
    "# Riduciamo 'positive' a un numero intermedio, ad esempio 5000\n",
    "under_sampler = RandomUnderSampler(sampling_strategy={'positive': 5000}, random_state=42)\n",
    "X_under, y_under = under_sampler.fit_resample(X_tfidf, df['Sentiment'])\n",
    "\n",
    "# 📌 2️⃣ **Upsampling delle classi negative e neutrali con SMOTE**\n",
    "# Portiamo 'negative' e 'neutral' a circa 5000 ciascuno\n",
    "smote = SMOTE(sampling_strategy={'negative': 5000, 'neutral': 5000}, random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X_under, y_under)\n",
    "\n",
    "# 📌 Stampare la distribuzione delle classi DOPO il bilanciamento\n",
    "print(\"\\n🔹 Distribuzione classi DOPO il bilanciamento:\")\n",
    "print(pd.Series(y_balanced).value_counts())\n",
    "\n",
    "# 📌 Divisione in training e test set\n",
    "X_train_sent, X_test_sent, y_train_sent, y_test_sent = train_test_split(\n",
    "    X_balanced, y_balanced, test_size=0.2, stratify=y_balanced, random_state=42\n",
    ")\n",
    "\n",
    "# 📌 Visualizzazione della distribuzione prima e dopo il bilanciamento\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# 📌 Grafico PRIMA del bilanciamento\n",
    "df['Sentiment'].value_counts().plot(kind='bar', color=['skyblue', 'lightgreen', 'salmon'], ax=axes[0])\n",
    "axes[0].set_title('Distribuzione PRIMA del bilanciamento')\n",
    "axes[0].set_xlabel('Sentiment')\n",
    "axes[0].set_ylabel('Numero di Campioni')\n",
    "\n",
    "# 📌 Grafico DOPO il bilanciamento\n",
    "pd.Series(y_balanced).value_counts().plot(kind='bar', color=['skyblue', 'lightgreen', 'salmon'], ax=axes[1])\n",
    "axes[1].set_title('Distribuzione DOPO il bilanciamento')\n",
    "axes[1].set_xlabel('Sentiment')\n",
    "axes[1].set_ylabel('Numero di Campioni')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 📌 Stampare le dimensioni finali dei dataset\n",
    "print(f\"\\n🔹 Dimensione Training Set: {X_train_sent.shape}\")\n",
    "print(f\"🔹 Dimensione Test Set: {X_test_sent.shape}\")"
   ],
   "id": "42fdb943a55a5885"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Naive Bayes",
   "id": "1b41be6962cf41c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 📌 Normalizzazione per migliorare Naïve Bayes\n",
    "normalizer = Normalizer()\n",
    "X_train_sent = normalizer.fit_transform(X_train_sent)\n",
    "X_test_sent = normalizer.transform(X_test_sent)\n",
    "\n",
    "# 📌 Ottimizzazione di `alpha` per MultinomialNB\n",
    "param_grid = {'alpha': [0.1, 0.5, 1.0, 5, 10]}\n",
    "grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5, scoring='f1_weighted')\n",
    "grid_search.fit(X_train_sent, y_train_sent)\n",
    "\n",
    "# 📌 Miglior valore di alpha trovato\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "print(f\"🔹 Miglior alpha trovato: {best_alpha}\")\n",
    "\n",
    "# 📌 Addestramento del modello con il miglior `alpha`\n",
    "nb_classifier = ComplementNB(alpha=best_alpha)\n",
    "nb_classifier.fit(X_train_sent, y_train_sent)\n",
    "\n",
    "# 📌 Previsioni sul test set\n",
    "y_pred_nb = nb_classifier.predict(X_test_sent)\n",
    "\n",
    "# 📌 Valutazione del modello\n",
    "print(\"🔹 Naïve Bayes Performance:\")\n",
    "print(classification_report(y_test_sent, y_pred_nb))\n",
    "\n"
   ],
   "id": "520cc433d0a97eb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# 📌 Matrice di confusione\n",
    "cm = confusion_matrix(y_test_sent, y_pred_nb)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])\n",
    "plt.xlabel(\"Predetto\")\n",
    "plt.ylabel(\"Reale\")\n",
    "plt.title(\"Matrice di Confusione - Naïve Bayes\")\n",
    "plt.show()"
   ],
   "id": "1d8f56cc5ffa8f4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Logistic Regression",
   "id": "7d42ac11999db64b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 📌 Standardizzazione dei dati\n",
    "scaler = StandardScaler(with_mean=False) # per gestire le matrici sparse geenrate da TF-IDF\n",
    "X_train_scaled = scaler.fit_transform(X_train_sent)  # Usa il dataset bilanciato\n",
    "X_test_scaled = scaler.transform(X_test_sent)  # Mantiene la scala nel test set\n",
    "\n",
    "# 📌 Addestramento del modello Logistic Regression\n",
    "log_reg = LogisticRegression(class_weight='balanced', max_iter=500)  # Maggiori iterazioni per stabilità\n",
    "log_reg.fit(X_train_scaled, y_train_sent)\n",
    "\n",
    "# 📌 Previsioni\n",
    "y_pred_log = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# 📌 Valutazione\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(classification_report(y_test_sent, y_pred_log))"
   ],
   "id": "487f1bfef3f2ace2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "SVM",
   "id": "6a2516777aa1a93f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 📌 **Standardizzazione con StandardScaler**\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)  # Evita problemi con matrici sparse TF-IDF\n",
    "X_train_scaled = scaler.fit_transform(X_train_sent)\n",
    "X_test_scaled = scaler.transform(X_test_sent)\n",
    "\n",
    "# 📌 **Definizione della griglia di parametri per SVM**\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Parametro di regolarizzazione\n",
    "    'gamma': [0.01, 0.1, 1, 'scale', 'auto'],  # Parametro di kernel\n",
    "    'kernel': ['rbf', 'linear']  # Manteniamo il kernel RBF\n",
    "}\n",
    "\n",
    "# 📌 **Ricerca dei migliori iperparametri con GridSearchCV**\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(class_weight='balanced'),\n",
    "    param_grid,\n",
    "    scoring='f1_weighted',\n",
    "    cv=3,\n",
    "    n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train_sent)\n",
    "\n",
    "# 📌 **Migliori parametri trovati**\n",
    "print(f\"🔹 Migliori parametri trovati: {grid_search.best_params_}\")\n",
    "\n",
    "# 📌 **Usiamo il miglior modello trovato**\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# 📌 **Previsioni**\n",
    "svm_predictions_rbf = best_svm.predict(X_test_scaled)\n",
    "\n",
    "# 📌 **Valutazione del modello**\n",
    "print(\"🔹 Report delle performance (SVM con GridSearch e StandardScaler):\")\n",
    "print(classification_report(y_test_sent, svm_predictions_rbf))"
   ],
   "id": "59883d7a8a25e895"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Word2Vec + Padding",
   "id": "9effc6644444abb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gensim.downloader as api\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "def get_word2vec_sequence(comment):\n",
    "    tokens = word_tokenize(comment)\n",
    "    vectors = [word2vec_model[word] for word in tokens if word in word2vec_model]\n",
    "    return np.array(vectors, dtype=np.float32) if vectors else np.zeros((1, 300), dtype=np.float32)\n",
    "\n",
    "word2vec_sequences = [get_word2vec_sequence(comment) for comment in df['Processed_Comment']]\n",
    "\n",
    "sequence_lengths = [seq.shape[0] for seq in word2vec_sequences]\n",
    "optimal_length = int(np.mean(sequence_lengths) + np.std(sequence_lengths))\n",
    "\n",
    "word2vec_sequences_padded = pad_sequences(word2vec_sequences, maxlen=optimal_length, dtype='float32', padding='post', truncating='post')\n",
    "\n",
    "X_word2vec = np.array(word2vec_sequences_padded, dtype=np.float32)\n",
    "\n",
    "sentiment_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df['Sentiment_Numeric'] = df['Sentiment'].map(sentiment_mapping)\n",
    "\n",
    "print(\"🔹 Distribuzione classi PRIMA del bilanciamento:\")\n",
    "print(pd.Series(df['Sentiment_Numeric']).value_counts())\n",
    "\n",
    "X_word2vec_flat = X_word2vec.reshape(X_word2vec.shape[0], -1) \n",
    "under_sampler = RandomUnderSampler(sampling_strategy={2: 5000}, random_state=42)\n",
    "X_under, y_under = under_sampler.fit_resample(X_word2vec_flat, df['Sentiment_Numeric'])\n",
    "\n",
    "smote = SMOTE(sampling_strategy={0: 5000, 1: 5000}, random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X_under, y_under)\n",
    "\n",
    "print(\"\\n🔹 Distribuzione classi DOPO il bilanciamento:\")\n",
    "print(pd.Series(y_balanced).value_counts())\n",
    "\n",
    "# 📌 **Ripristinare la forma originale per LSTM** (N, timesteps, embedding_dim)\n",
    "X_balanced = X_balanced.reshape(-1, optimal_length, 300)\n",
    "\n",
    "# 📌 **Suddivisione del dataset bilanciato in Training e Test**\n",
    "X_train_sent_seq, X_test_sent_seq, y_train_sent, y_test_sent = train_test_split(\n",
    "    X_balanced, y_balanced, test_size=0.2, stratify=y_balanced, random_state=42\n",
    ")\n",
    "\n",
    "# 📌 Controllare le dimensioni\n",
    "print(f\"Forma di X_train_sent_seq: {X_train_sent_seq.shape}\")\n",
    "print(f\"Forma di X_test_sent_seq: {X_test_sent_seq.shape}\")\n",
    "print(f\"Distribuzione classi train:\\n{np.bincount(y_train_sent)}\")\n",
    "print(f\"Distribuzione classi test:\\n{np.bincount(y_test_sent)}\")"
   ],
   "id": "85359d203994b9e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LSTM",
   "id": "123e6731978688e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# 📌 Creazione del modello LSTM ottimizzato per evitare overfitting\n",
    "# =========================\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    Masking(mask_value=0.0, input_shape=(optimal_length, 300)),  # Ignora i padding\n",
    "    LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3),  # 🔹 Dropout nella LSTM\n",
    "    LSTM(32, dropout=0.3, recurrent_dropout=0.3),  # 🔹 Strato LSTM più piccolo\n",
    "    Dropout(0.3),  # 🔹 Dropout tra le LSTM e il Dense\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),  # 🔹 L2 Regularization\n",
    "    Dropout(0.3),  # 🔹 Dropout anche nel Fully Connected\n",
    "    Dense(3, activation='softmax')  # 🔹 Output con 3 classi\n",
    "])\n",
    "\n",
    "\"\"\"lstm_model = Sequential([\n",
    "    Masking(mask_value=0.0, input_shape=(optimal_length, 300)),  # Ignora i padding\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\"\"\"\n",
    "\n",
    "# 📌 Compilazione del modello\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 📌 Calcolo dei pesi delle classi\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_sent), y=y_train_sent)\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# 📌 Early Stopping con patience maggiore\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "\n",
    "# 📌 Addestramento con batch_size maggiore\n",
    "history = lstm_model.fit(X_train_sent_seq, y_train_sent,\n",
    "                         epochs=20,  # 🔹 Aumentato per stabilità\n",
    "                         batch_size=64,  # 🔹 Batch più grande per aggiornamenti più stabili\n",
    "                         validation_data=(X_test_sent_seq, y_test_sent),\n",
    "                         callbacks=[early_stopping])  # 🔹 Usare pesi di classe"
   ],
   "id": "fd370070b145f0aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Grafico di Accuracy e Loss durante l’addestramento",
   "id": "a300428c3c5b70ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 📌 Ottenere i dati dall'addestramento\n",
    "history = lstm_model.history.history\n",
    "\n",
    "# 📌 Plot della accuracy\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training & Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 📌 Plot della loss\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history['loss'], label='Training Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "id": "4568a0195996f8ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 📌 Previsioni sul test set\n",
    "y_pred_lstm = np.argmax(lstm_model.predict(X_test_sent_seq), axis=1)  # Converte probabilità in classi\n",
    "\n",
    "# 📌 Generare il classification report\n",
    "report = classification_report(y_test_sent, y_pred_lstm, target_names=['negative', 'neutral', 'positive'])\n",
    "\n",
    "# 📌 Stampare il report\n",
    "print(\"LSTM Performance:\\n\", report)"
   ],
   "id": "8274abb178029cb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# 📌 Creare la matrice di confusione\n",
    "cm = confusion_matrix(y_test_sent, y_pred_lstm)\n",
    "\n",
    "# 📌 Plot della matrice di confusione\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Matrice di Confusione - LSTM Sentiment Analysis\")\n",
    "plt.show()"
   ],
   "id": "ce24e0aa18eb7635"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Transformer",
   "id": "42e0b12cc6a4d176"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "# Controllare la disponibilità della GPU\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ],
   "id": "c457e946546dddd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# 📌 Controllare e impostare il dispositivo su GPU CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🔹 Using device: {device}\")\n",
    "\n",
    "# 📌 Mappare i sentimenti in numeri\n",
    "sentiment_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df['Sentiment_Numeric'] = df['Sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# 📌 Stampare la distribuzione delle classi PRIMA del bilanciamento\n",
    "print(\"🔹 Distribuzione classi PRIMA del bilanciamento:\")\n",
    "print(df['Sentiment_Numeric'].value_counts())\n",
    "\n",
    "# 📌 Suddividere il dataset bilanciato in training e test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['Processed_Comment'].tolist(), df['Sentiment_Numeric'].tolist(),\n",
    "    test_size=0.2, stratify=df['Sentiment_Numeric'], random_state=42\n",
    ")\n",
    "\n",
    "# 📌 Caricare il Tokenizer di BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 📌 Tokenizzare i testi per BERT\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding='max_length', max_length=70, return_tensors='pt')\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding='max_length', max_length=70, return_tensors='pt')\n",
    "\n",
    "# 📌 Convertire gli encodings in NumPy per poter applicare SMOTE\n",
    "X_train = train_encodings['input_ids'].numpy()\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "# 📌 **Bilanciamento Dopo la Tokenizzazione**\n",
    "# 🔹 1️⃣ **Downsampling della classe `positive` (2) → Ridotta a 5000 campioni**\n",
    "under_sampler = RandomUnderSampler(sampling_strategy={2: 5000}, random_state=42)\n",
    "X_under, y_under = under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# 🔹 2️⃣ **Upsampling delle classi `negative` (0) e `neutral` (1) con SMOTE → Portate a 5000 campioni**\n",
    "smote = SMOTE(sampling_strategy={0: 5000, 1: 5000}, random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X_under, y_under)\n",
    "\n",
    "# 📌 Stampare la distribuzione delle classi DOPO il bilanciamento\n",
    "print(\"\\n🔹 Distribuzione classi DOPO il bilanciamento:\")\n",
    "print(pd.Series(y_balanced).value_counts())\n",
    "\n",
    "# 📌 **Convertire i dati bilanciati in Tensor per PyTorch**\n",
    "X_balanced_tensor = torch.tensor(X_balanced, dtype=torch.long)\n",
    "y_balanced_tensor = torch.tensor(y_balanced, dtype=torch.long)\n",
    "\n",
    "# 📌 **Classe Dataset Corretto**\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, input_ids, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"input_ids\": self.input_ids[idx], \"labels\": self.labels[idx]}\n",
    "\n",
    "# 📌 Creazione dei dataset\n",
    "train_dataset = SentimentDataset(X_balanced_tensor, y_balanced_tensor)\n",
    "test_dataset = SentimentDataset(test_encodings[\"input_ids\"], torch.tensor(test_labels))\n",
    "\n",
    "# 📌 Configura BERT con dropout più alto\n",
    "config = BertConfig.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=3,\n",
    "    hidden_dropout_prob=0.3,\n",
    "    attention_probs_dropout_prob=0.3\n",
    ")\n",
    "\n",
    "# 📌 Caricare il modello pre-addestrato di BERT per classificazione\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "model.to(device)\n",
    "\n",
    "# 📌 Definire l'ottimizzatore AdamW con weight decay\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# 📌 Aumentare il batch size per sfruttare la GPU\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 📌 Funzione di Training Personalizzata\n",
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    class_weights = torch.tensor([1.0 / 0.33, 1.0 / 0.33, 1.0 / 0.33]).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    results = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader, leave=True)\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch in loop:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=inputs[\"input_ids\"])\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_description(f'Epoch {epoch + 1}')\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_training_loss = total_loss / len(train_loader)\n",
    "        val_loss, val_accuracy = evaluate_model(model, val_loader)\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        results.append({\n",
    "            \"Epoca\": f\"Epoca {epoch + 1}\",\n",
    "            \"Training Loss\": round(avg_training_loss, 4),\n",
    "            \"Validation Loss\": round(val_loss, 4),\n",
    "            \"Validation Accuracy\": round(val_accuracy, 4),\n",
    "            \"Training Time\": time.strftime(\"%H:%M:%S\", time.gmtime(epoch_time))\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df)\n",
    "    return results_df\n",
    "\n",
    "# 📌 Funzione di Valutazione\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=inputs[\"input_ids\"])\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = total_loss / len(val_loader)\n",
    "    accuracy = correct / total\n",
    "    return avg_val_loss, accuracy\n",
    "\n",
    "# 📌 Avviare l'addestramento\n",
    "results_df = train_model(model, train_loader, val_loader, epochs=10)\n",
    "\n",
    "# 📌 Salva il DataFrame con i risultati\n",
    "results_df.to_csv(\"training_results.csv\", index=False)\n",
    "\n",
    "# 📌 Valutazione sul set di test\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "        labels = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "        outputs = model(input_ids=inputs[\"input_ids\"])\n",
    "        preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "# 📌 Salva le previsioni e i veri label\n",
    "np.save(\"predictions.npy\", np.array(all_preds))\n",
    "np.save(\"true_labels.npy\", np.array(all_labels))"
   ],
   "id": "c23b4ee13523f583"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# 📌 Carica i dati salvati\n",
    "results_df = pd.read_csv(\"training_results.csv\")\n",
    "all_preds = np.load(\"predictions.npy\")\n",
    "all_labels = np.load(\"true_labels.npy\")\n",
    "\n",
    "# 📌 Grafico Loss durante il Training\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(results_df[\"Epoca\"], results_df[\"Training Loss\"], label=\"Training Loss\", marker=\"o\")\n",
    "plt.plot(results_df[\"Epoca\"], results_df[\"Validation Loss\"], label=\"Validation Loss\", marker=\"o\")\n",
    "plt.xlabel(\"Epoca\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.show()\n",
    "\n",
    "# 📌 Grafico Accuracy durante il Training\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(results_df[\"Epoca\"], results_df[\"Validation Accuracy\"], label=\"Validation Accuracy\", marker=\"o\", color=\"green\")\n",
    "plt.xlabel(\"Epoca\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Validation Accuracy Trend\")\n",
    "plt.show()\n",
    "\n",
    "# 📌 Matrice di Confusione\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# 📌 Report di Classificazione\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(all_labels, all_preds, target_names=labels))"
   ],
   "id": "10e8fc636a473b91"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
